{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job Example\n",
    "* Requires you to have a spark master running\n",
    "    * Install Spark locally on your node (https://spark.apache.org/docs/latest/spark-standalone.html)\n",
    "    * Start up a spark master instance on your node: `$ $(SPARK_HOME)/sbin/start-master.sh`\n",
    "    * Start up at least one slave: `$ $(SPARK_HOME)/sbin/start-worker.sh spark://localhost:7077`\n",
    "    * Verify that your master and one slave is up at http://localhost:8088\n",
    "    * NOTE: If you are using this with a Spark standalone cluster you must ensure that the installed version (including minor version) matches the PySpark version or you may experience odd errors.\n",
    "    * NOTE: You may run into issues with ports if you're on a corp network or VPN.  See https://stackoverflow.com/questions/52133731/how-to-solve-cant-assign-requested-address-service-sparkdriver-failed-after\n",
    "* Requires you to have aws configured to allow pushing files to S3\n",
    "    * Install the awscli: `$ pip install awscli` \n",
    "    * Configure your credentials: `$ aws configure`\n",
    "\n",
    "## Illustrates how to:\n",
    "* Submit a Spark job from a task\n",
    "* Have the Spark job read from a bundle with s3 paths\n",
    "* Have the Spark job write to s3 paths contained in the output bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import disdat.api as api\n",
    "from disdatluigi.api import apply\n",
    "from disdat.api import Bundle\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import luigi\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "\n",
    "%aimport pipelines.spark_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a bundle with s3 paths\n",
    "NOTE: Requires a remote context to push to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already bound to remote at s3://disdat-cdo-prd/\n",
      "Pushed committed bundle None uuid 0068f9ce-1900-4bc0-84ed-459b2b6e0246 to remote s3://disdat-cdo-prd/context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<disdat.api.Bundle at 0x7fcc43021a60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context = 'example-context'\n",
    "remote_context_url = 's3://disdat-cdo-prd/'  # <------ Replace with the location of your Disdat contexts on S3\n",
    "api.context(data_context)\n",
    "api.remote(data_context, data_context, remote_context_url)\n",
    "\n",
    "with Bundle(data_context, name=\"s3_files\") as b:\n",
    "    f1 = b.get_file(\"file_1.txt\")\n",
    "    with open(f1, mode='w') as f:\n",
    "        f.write(\"This is our first file!\")\n",
    "    b.add_data(f1)\n",
    "\n",
    "# Push and remove the local file\n",
    "b.commit().push(delocalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark_master = 'spark://localhost:7077'   # Fill in your spark URL (available at web page localhost:8080)\n",
    "app_name = \"testapp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if RunSparkJob(spark_master=spark://intuitdepe1ea6:7077, app_name=testapp, input_bundle_name=s3_files) is complete\n",
      "DEBUG: Checking if ExternalDepTask(uuid=0068f9ce-1900-4bc0-84ed-459b2b6e0246, processing_name=_d41d8cd98f_d41d8cd98f) is complete\n",
      "INFO: Informed scheduler that task   RunSparkJob_testapp_s3_files_spark___intuitde_75ab6e1f73   has status   PENDING\n",
      "INFO: Informed scheduler that task   ExternalDepTask__d41d8cd98f_d41d_0068f9ce_1900_4b_9ccb2167db   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 665] Worker Worker(salt=523456111, workers=1, host=intuitdepe1ea6, username=kyocum, pid=665) running   RunSparkJob(spark_master=spark://intuitdepe1ea6:7077, app_name=testapp, input_bundle_name=s3_files)\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kyocum/bin/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kyocum/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kyocum/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07a5cf4c-009c-4d66-9f0e-ea59e73f7de4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 133ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-07a5cf4c-009c-4d66-9f0e-ea59e73f7de4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "22/01/19 17:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark reading file:s3a://disdat-cdo-prd/context/example-context/objects/0068f9ce-1900-4bc0-84ed-459b2b6e0246/file_1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/19 17:29:42 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 0, lines with b: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['context/example-context/objects/43cbbed8-fee9-4283-8047-a4b74bf947e8/job_output/_SUCCESS', 'context/example-context/objects/43cbbed8-fee9-4283-8047-a4b74bf947e8/job_output/part-00000-7a88870d-4124-4c07-80bf-b4c1ee249b4c-c000.snappy.parquet', 'context/example-context/objects/43cbbed8-fee9-4283-8047-a4b74bf947e8/job_output/part-00011-7a88870d-4124-4c07-80bf-b4c1ee249b4c-c000.snappy.parquet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 665] Worker Worker(salt=523456111, workers=1, host=intuitdepe1ea6, username=kyocum, pid=665) done      RunSparkJob(spark_master=spark://intuitdepe1ea6:7077, app_name=testapp, input_bundle_name=s3_files)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   RunSparkJob_testapp_s3_files_spark___intuitde_75ab6e1f73   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=523456111, workers=1, host=intuitdepe1ea6, username=kyocum, pid=665) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 ExternalDepTask(...)\n",
      "* 1 ran successfully:\n",
      "    - 1 RunSparkJob(...)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed committed bundle None uuid 43cbbed8-fee9-4283-8047-a4b74bf947e8 to remote s3://disdat-cdo-prd/context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success': True, 'did_work': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(\"example-context\",\n",
    "          pipelines.spark_tasks.RunSparkJob,\n",
    "         params={'spark_master': spark_master,\n",
    "                'app_name': app_name},\n",
    "         incremental_push=True,\n",
    "         force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
